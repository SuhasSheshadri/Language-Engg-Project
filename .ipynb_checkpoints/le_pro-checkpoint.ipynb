{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk, re, pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, MWETokenizer\n",
    "from nltk.stem import porter, WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation,  TruncatedSVD, NMF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.preprocessing  import  StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_main = pd.read_csv('/Users/Rithika/ted_main.csv')\n",
    "ted_trans = pd.read_csv('/Users/Rithika/transcripts.csv')    \n",
    "ted_all = pd.merge(ted_trans,right=ted_main,on='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ted_all.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(ted_all, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_all['id'] = ted_all.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ted_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks = ted_all['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to clean one document only\n",
    "\n",
    "def clean_text_onedoc(text):\n",
    "\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"','!', ';',\\\n",
    "             '.\\'\"','[',']','—',\".\\'\", 'ok','okay','yeah','ya','stuff', ' 000 ',' em ','get','got',\\\n",
    "             ' oh ','la','was','wa','?','like','go',' le ',' ca ',' I ',\" ? \",\"s\", \" t \",\"ve\",\"re\"]\n",
    "    \n",
    "    for word in wordpunct_tokenize(text): \n",
    "        cleaned = []\n",
    "        if word.lower() not in stop:\n",
    "            keepw = lemmizer.lemmatize(word)\n",
    "            if keepw.lower not in stop:\n",
    "                cleaned.append(keepw.lower())\n",
    "                \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes in a corpus of documents and cleans. ONly works with multiple docs for now\n",
    "    \n",
    "    1. remove parentheticals\n",
    "    2. tokenize into words using wordpunct\n",
    "    3. lowercase and remove stop words\n",
    "    4. lemmatize \n",
    "    5. lowercase and remove stop words\n",
    "    \n",
    "    \n",
    "    OUT: cleaned text = a list (documents) of lists (cleaned word in each doc)\n",
    "    \"\"\"\n",
    "\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    #stemmer = porter.PorterStemmer()\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"','!', ';','♫♫','♫',\\\n",
    "             '.\\'\"','[',']','—',\".\\'\", 'ok','okay','yeah','ya','stuff', ' 000 ',' em ',\\\n",
    "             ' oh ','thank','thanks','la','was','wa','?','like','go',' le ',' ca ',' I ',\" ? \",\"s\", \" t \",\"ve\",\"re\"]\n",
    "    #stop = set(stop)\n",
    "\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        \n",
    "        # remove parentheticals\n",
    "        clean_parens = re.sub(r'\\([^)]*\\)', ' ', post)\n",
    "        \n",
    "        # tokenize into words\n",
    "        for word  in wordpunct_tokenize(clean_parens):  \n",
    "            \n",
    "            # lowercase and throw out any words in stop words\n",
    "            if word.lower() not in stop:\n",
    "            \n",
    "                # lemmatize  to roots\n",
    "                low_word = lemmizer.lemmatize(word)  \n",
    "\n",
    "                # stem and lowercase ( an alternative to lemmatize)\n",
    "                #low_word = stemmer.stem(root.lower())  \n",
    "            \n",
    "                # keep if not in stopwords (yes, again)\n",
    "                if low_word.lower() not in stop: \n",
    "                    \n",
    "                    # put into a list of words for each document\n",
    "                    cleaned_words.append(low_word.lower())\n",
    "        \n",
    "        # keep corpus of cleaned words for each document    \n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_talks = clean_text(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_talks.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(cleaned_talks, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\n",
    "c_vectorizer = CountVectorizer(ngram_range=(1,3), \n",
    "                             stop_words='english', \n",
    "                             max_df = 0.6, \n",
    "                             max_features=10000)\n",
    "\n",
    "t_vectorizer = TfidfVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "c_vectorizer.fit(cleaned_talks)\n",
    "# finally, call `transform` to convert text to a bag of words\n",
    "c_x = c_vectorizer.transform(cleaned_talks)\n",
    "\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "t_vectorizer.fit(cleaned_talks)\n",
    "# finally, call `transform` to convert text to a bag of words\n",
    "t_x = t_vectorizer.transform(cleaned_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open metadata and cleaned talks\n",
    "\n",
    "with open('ted_all.pkl', 'rb') as picklefile:\n",
    "    ted_all = pickle.load(picklefile)\n",
    "\n",
    "    \n",
    "with open('cleaned_talks.pkl', 'rb') as picklefile:\n",
    "    cleaned_talks = pickle.load(picklefile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mod_lda(data,topics=5,iters=10,ngram_min=1, ngram_max=3, max_df=0.6, max_feats=5000):\n",
    "    \n",
    "    \"\"\" vectorizer - turn words into numbers for each document(rows)\n",
    "    then use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_min,ngram_max), \n",
    "                             stop_words='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features=max_feats)\n",
    "    \n",
    "      \n",
    "    #  `fit (train), then transform` to convert text to a bag of words\n",
    "\n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=topics,\n",
    "                                    max_iter=iters,\n",
    "                                    random_state=42,\n",
    "                                    learning_method='online',\n",
    "                                    n_jobs=-1)\n",
    "    \n",
    "    lda_dat = lda.fit_transform(vect_data)\n",
    "    \n",
    "    \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    display_topics(lda, vectorizer.get_feature_names(),20)\n",
    "    \n",
    "    \n",
    "    return vectorizer, vect_data, lda, lda_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "brain cell body patient blood neuron heart tissue organ surgery stem disease skin muscle bone sugar stem cell arm mouse animal\n",
      "Topic  1\n",
      "school child kid student teacher education girl parent learning learn class high old teach family young year old college high school percent\n",
      "Topic  2\n",
      "animal specie tree fish forest plant water nature bird coral 000 bee creature insect ant river living area natural dinosaur\n",
      "Topic  3\n",
      "country percent global million china united billion 000 war economy states united states india dollar growth economic climate africa population number\n",
      "Topic  4\n",
      "earth planet water ocean sea solar ice air mars space sun energy 000 mile foot cloud surface atmosphere temperature wind\n",
      "Topic  5\n",
      "government company country political social society power community business group law organization leader africa public value money state issue democracy\n",
      "Topic  6\n",
      "rule compassion moral flag bread zero stage non sum bad understand news good news transformation nature enemy win try hate treated\n",
      "Topic  7\n",
      "health care percent patient doctor money family medical hospital baby child dollar help disease death choice number treatment worker month\n",
      "Topic  8\n",
      "food water product energy oil percent market company plant farmer business waste dollar eat industry production produce buy cost solution\n",
      "Topic  9\n",
      "game play video playing player real toy video game win tail played head ball team puzzle try dynamic virtual fun learn\n",
      "Topic  10\n",
      "city building design space project community street built new york york house build public architecture live neighborhood map home urban create\n",
      "Topic  11\n",
      "cancer gene dna disease drug cell virus molecule genome genetic tumor bacteria protein vaccine chemical biology science trial medicine breast\n",
      "Topic  12\n",
      "car robot machine technology computer design build sort material turn using model small little bit light working fly power try speed\n",
      "Topic  13\n",
      "internet book guy word phone sort medium computer number online company information page read google interesting web 000 million picture\n",
      "Topic  14\n",
      "brain word mind self science experience language feel example sense person course understand reason study thinking answer true believe wrong\n",
      "Topic  15\n",
      "woman universe black men star space physic light galaxy sex particle theory matter gender female hole law telescope quantum male\n",
      "Topic  16\n",
      "ca ted chris em anderson bg yes mr copy mind future feel thousand term short happen reality quite lot people richard\n",
      "Topic  17\n",
      "story woman love man family old feel friend home told men wanted child mother girl father moment night guy young\n",
      "Topic  18\n",
      "data information technology network device computer using algorithm example image digital brain phone tool eye looking process face pattern area\n",
      "Topic  19\n",
      "art music sound image piece artist film sort story play experience moment object create song eye feel painting hand color\n"
     ]
    }
   ],
   "source": [
    "vect_mod, vect_data, lda_mod, lda_data = topic_mod_lda(cleaned_talks,\n",
    "                                                       topics=20,\n",
    "                                     iters=100,\n",
    "                                     ngram_min=1, \n",
    "                                     ngram_max=2, \n",
    "                                     max_df=0.5, \n",
    "                                     max_feats=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2467,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_ind = np.argmax(lda_data, axis=1)\n",
    "topic_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=topic_ind\n",
    "topic_labels = pd.DataFrame(y)\n",
    "topic_names = topic_labels\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save text labels to csv and pkl for plotting\n",
    "\n",
    "topic_names.to_csv('topic_names.csv')\n",
    "\n",
    "with open('topic_names.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(topic_names, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(first_article,num_of_recs,topics,ted_data, model, vectorizer, training_vectors):\n",
    "    \n",
    "    new_vec = model.transform(\n",
    "        vectorizer.transform([first_article]))\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=num_of_recs, metric='cosine', algorithm='brute')\n",
    "    nn.fit(training_vectors)\n",
    "    \n",
    "    results = nn.kneighbors(new_vec)\n",
    "    \n",
    "    recommend_list = results[1][0]\n",
    "    scores = results[0]\n",
    "                       \n",
    "    ss = np.array(scores).flat\n",
    "    for i, resp in enumerate(recommend_list):\n",
    "        print('\\n--- ID ---\\n', + resp)\n",
    "        print('--- distance ---\\n', + ss[i])  \n",
    "        print('--- topic ---')\n",
    "        print(topics.iloc[resp,0])\n",
    "        print(ted_data.iloc[resp,1])\n",
    "        print('--- teds tags ---')\n",
    "        print(ted_data.iloc[resp,-3])\n",
    "        \n",
    "    return recommend_list, ss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ID ---\n",
      " 804\n",
      "--- distance ---\n",
      " 0.0\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/charles_limb_your_brain_on_improv\n",
      "\n",
      "--- teds tags ---\n",
      "['TEDx', 'brain', 'creativity', 'entertainment', 'music', 'science', 'technology']\n",
      "\n",
      "--- ID ---\n",
      " 1752\n",
      "--- distance ---\n",
      " 0.08891830375260179\n",
      "--- topic ---\n",
      "technology, privacy\n",
      "https://www.ted.com/talks/nancy_kanwisher_the_brain_is_a_swiss_army_knife\n",
      "\n",
      "--- teds tags ---\n",
      "['brain', 'neuroscience', 'visualizations']\n",
      "\n",
      "--- ID ---\n",
      " 2122\n",
      "--- distance ---\n",
      " 0.09103100669662334\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/uri_hasson_this_is_your_brain_on_communication\n",
      "\n",
      "--- teds tags ---\n",
      "['brain', 'cognitive science', 'collaboration', 'communication', 'language', 'mind', 'neuroscience', 'science', 'speech']\n",
      "\n",
      "--- ID ---\n",
      " 1255\n",
      "--- distance ---\n",
      " 0.11806924992412071\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/sarah_jayne_blakemore_the_mysterious_workings_of_the_adolescent_brain\n",
      "\n",
      "--- teds tags ---\n",
      "['aging', 'biology', 'brain', 'children', 'cognitive science', 'consciousness', 'intelligence', 'medical imaging', 'mind', 'neuroscience', 'parenting', 'science', 'sociology', 'youth']\n",
      "\n",
      "--- ID ---\n",
      " 1044\n",
      "--- distance ---\n",
      " 0.11818453069303303\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/antonio_damasio_the_quest_to_understand_consciousness\n",
      "\n",
      "--- teds tags ---\n",
      "['Senses', 'brain', 'cognitive science', 'consciousness', 'decision-making', 'identity', 'medical imaging', 'medical research', 'memory', 'mental health', 'mind', 'neuroscience', 'science', 'self']\n",
      "\n",
      "--- ID ---\n",
      " 2106\n",
      "--- distance ---\n",
      " 0.12142044612267877\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/chris_anderson_teds_secret_to_great_public_speaking\n",
      "\n",
      "--- teds tags ---\n",
      "['brain', 'communication', 'language', 'mind', 'speech']\n",
      "\n",
      "--- ID ---\n",
      " 474\n",
      "--- distance ---\n",
      " 0.13322416720911878\n",
      "--- topic ---\n",
      "technology, privacy\n",
      "https://www.ted.com/talks/tom_wujec_on_3_ways_the_brain_creates_meaning\n",
      "\n",
      "--- teds tags ---\n",
      "['brain', 'creativity', 'design', 'presentation', 'technology', 'visualizations']\n",
      "\n",
      "--- ID ---\n",
      " 820\n",
      "--- distance ---\n",
      " 0.13358243327289354\n",
      "--- topic ---\n",
      "technology, privacy\n",
      "https://www.ted.com/talks/ariel_garten_know_thyself_with_a_brain_scanner\n",
      "\n",
      "--- teds tags ---\n",
      "['TEDx', 'neuroscience', 'psychology', 'science', 'technology']\n",
      "\n",
      "--- ID ---\n",
      " 981\n",
      "--- distance ---\n",
      " 0.15869634838512758\n",
      "--- topic ---\n",
      "politics\n",
      "https://www.ted.com/talks/sunni_brown\n",
      "\n",
      "--- teds tags ---\n",
      "['business', 'creativity', 'culture', 'presentation']\n",
      "\n",
      "--- ID ---\n",
      " 1857\n",
      "--- distance ---\n",
      " 0.17824341143007505\n",
      "--- topic ---\n",
      "technology, privacy\n",
      "https://www.ted.com/talks/david_eagleman_can_we_create_new_senses_for_humans\n",
      "\n",
      "--- teds tags ---\n",
      "['Blindness', 'Senses', 'brain', 'technology']\n"
     ]
    }
   ],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804],10, topic_names, ted_all,\n",
    "                                       lda_mod, vect_mod, lda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
