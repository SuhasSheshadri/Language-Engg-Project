{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk, re, pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "#from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, MWETokenizer\n",
    "from nltk.stem import porter, WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_main = pd.read_csv('data/ted_main.csv')\n",
    "ted_trans = pd.read_csv('data/transcripts.csv')    \n",
    "ted_all = pd.merge(ted_trans,right=ted_main,on='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_all['id'] = ted_all.index\n",
    "print(len(ted_all))\n",
    "talks = ted_all['transcript']\n",
    "print(len(talks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes in a corpus of documents and cleans. ONly works with multiple docs for now\n",
    "    \n",
    "    1. remove parentheticals\n",
    "    2. tokenize into words using wordpunct\n",
    "    3. lowercase and remove stop words\n",
    "    4. lemmatize \n",
    "    5. lowercase and remove stop words\n",
    "    \n",
    "    \n",
    "    OUT: cleaned text = a list (documents) of lists (cleaned word in each doc)\n",
    "    \"\"\"\n",
    "\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "    #stemmer = porter.PorterStemmer()\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"','!', ';','♫♫','♫',\\\n",
    "             '.\\'\"','[',']','—',\".\\'\", 'ok','okay','yeah','ya','stuff', ' 000 ',' em ',\"ll\",\"didn\",\\\n",
    "             ' oh ','thank','thanks','la','was','wa','?','like','go',' le ',' ca ',' I ',\" ? \",\"s\", \" t \",\"ve\",\"re\"]\n",
    "\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for post in text:\n",
    "        cleaned_words = []\n",
    "        \n",
    "        # remove parentheticals\n",
    "        clean_parens = re.sub(r'\\([^)]*\\)', ' ', post)\n",
    "        \n",
    "        # tokenize into words\n",
    "        for word  in wordpunct_tokenize(clean_parens):  \n",
    "            \n",
    "            # lowercase and throw out any words in stop words\n",
    "            if word.lower() not in stop:\n",
    "            \n",
    "                # lemmatize  to roots\n",
    "                low_word = lemmizer.lemmatize(word)  \n",
    "\n",
    "                # stem and lowercase ( an alternative to lemmatize)\n",
    "                #low_word = stemmer.stem(root.lower())  \n",
    "            \n",
    "                # keep if not in stopwords (yes, again)\n",
    "                if low_word.lower() not in stop: \n",
    "                    \n",
    "                    # put into a list of words for each document\n",
    "                    cleaned_words.append(low_word.lower())\n",
    "        \n",
    "        # keep corpus of cleaned words for each document    \n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "cleaned_talks = clean_text(talks)\n",
    "print(\"Cleaned data in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(first_article,num_of_recs,topics,ted_data, model, vectorizer, training_vectors):\n",
    "    \n",
    "    new_vec = model.transform(\n",
    "        vectorizer.transform([first_article]))\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=num_of_recs, metric='cosine', algorithm='brute')\n",
    "    nn.fit(training_vectors)\n",
    "    \n",
    "    results = nn.kneighbors(new_vec)\n",
    "    \n",
    "    recommend_list = results[1][0]\n",
    "    scores = results[0]\n",
    "                       \n",
    "    ss = np.array(scores).flat       \n",
    "    for i, resp in enumerate(recommend_list):\n",
    "        print('\\nID: ', + resp)\n",
    "        print('Cosine Distance: ', + ss[i])  \n",
    "        print('Topics: ' + topics.iloc[resp,0])\n",
    "        print('URL: ' + ted_data.iloc[resp,1])\n",
    "        print(\"TED's original tags: \")\n",
    "        print(ted_data.iloc[resp,-3])\n",
    "        print(\"\\n------------------------\")\n",
    "        \n",
    "    return recommend_list, ss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mod_lda(data,topics=5,iters=10,ngram_min=1, ngram_max=3, max_df=0.6, max_feats=5000):\n",
    "    \n",
    "    \"\"\" vectorizer - turn words into numbers for each document(rows)\n",
    "    then use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_min,ngram_max), \n",
    "                             stop_words='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features=max_feats)\n",
    "    \n",
    "      \n",
    "    #  `fit (train), then transform` to convert text to a bag of words\n",
    "\n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=topics,\n",
    "                                    max_iter=iters,\n",
    "                                    random_state=42,\n",
    "                                    learning_method='online',\n",
    "                                    n_jobs=-1)\n",
    "    \n",
    "    lda_dat = lda.fit_transform(vect_data)\n",
    "    \n",
    "    \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    display_topics(lda, vectorizer.get_feature_names(),20)\n",
    "    \n",
    "    \n",
    "    return vectorizer, vect_data, lda, lda_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "vect_mod, vect_data, lda_mod, lda_data = topic_mod_lda(cleaned_talks,\n",
    "                                                       topics=20,\n",
    "                                                       iters=10,\n",
    "                                                       ngram_min=1, \n",
    "                                                       ngram_max=2, \n",
    "                                                       max_df=0.5, \n",
    "                                                       max_feats=2000)\n",
    "print(\"LDA done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(lda_data, axis=1)\n",
    "topic_labels = pd.DataFrame(topic_ind)\n",
    "topic_names = topic_labels\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804],10, topic_names, ted_all,\n",
    "                                       lda_mod, vect_mod, lda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mod_nmf(data, topics=5,iters=10,ngram_min=1, ngram_max=3, max_df=0.6, max_feats=5000):\n",
    "    \n",
    "    \"\"\" vectorizer - turn words into numbers for each document(rows)\n",
    "    then use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_min,ngram_max), \n",
    "                             stop_words='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features=max_feats)\n",
    "    \n",
    "    # call `fit` to build the vocabulary\n",
    "#     vectorizer.fit(data)\n",
    "    \n",
    "    # finally, call `transform` to convert text to a bag of words\n",
    "\n",
    "#     vect_data = vectorizer.transform(data)\n",
    "    \n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    nmf = NMF(n_components=topics,\n",
    "                max_iter=iters,\n",
    "                random_state=42)\n",
    "    \n",
    "    nmf_dat = nmf.fit_transform(vect_data)\n",
    "    \n",
    "    \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model_, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model_.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    display_topics(nmf, vectorizer.get_feature_names(),20)\n",
    "    \n",
    "    \n",
    "    return vectorizer, vect_data, nmf, nmf_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "vect_mod, vect_data, nmf_mod, nmf_data  = topic_mod_nmf(cleaned_talks,\n",
    "                                                             topics=20,\n",
    "                                                             iters=100,\n",
    "                                                             ngram_min=1, \n",
    "                                                             ngram_max=2, \n",
    "                                                             max_df=0.6, \n",
    "                                                             max_feats=2000)\n",
    "print(\"NMF done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(nmf_data, axis=1)\n",
    "topic_labels = pd.DataFrame(topic_ind)\n",
    "topic_names = topic_labels\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(first_article,num_of_recs,topics,ted_data, model, vectorizer, training_vectors):\n",
    "    \n",
    "    new_vec = model.transform(\n",
    "        vectorizer.transform([first_article]))\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=num_of_recs, metric='cosine', algorithm='brute')\n",
    "    nn.fit(training_vectors)\n",
    "    \n",
    "    results = nn.kneighbors(new_vec)\n",
    "    \n",
    "    recommend_list = results[1][0]\n",
    "    scores = results[0]\n",
    "                       \n",
    "    ss = np.array(scores).flat\n",
    "    for i, resp in enumerate(recommend_list):\n",
    "        print('\\n--- ID ---\\n', + resp)\n",
    "        print('--- distance ---\\n', + ss[i])  \n",
    "        print('--- topic ---')\n",
    "        print(topics.iloc[resp,0])\n",
    "        print(ted_data.iloc[resp,1])\n",
    "        print('--- teds tags ---')\n",
    "        print(ted_data.iloc[resp,-3])\n",
    "        \n",
    "    return recommend_list, ss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804],10, topic_names, ted_all,\n",
    "                                       nmf_mod, vect_mod, nmf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mod_lsa(data, topics=5,ngram_min=1, ngram_max=3, max_df=0.6, max_feats=5000):\n",
    "    \n",
    "    \"\"\" vectorizer - turn words into numbers for each document(rows)\n",
    "    then use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram_min,ngram_max), \n",
    "                             stop_words='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features=max_feats)\n",
    "    \n",
    "    # call `fit` to build the vocabulary\n",
    "   \n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    #stdScale = Normalizer()\n",
    "\n",
    "    #vect_scale = stdScale.fit_transform(vect_data)\n",
    "    \n",
    "    lsa = TruncatedSVD(n_components=topics,random_state=42)\n",
    "    \n",
    "    lsa_dat = lsa.fit_transform(vect_data)\n",
    "    \n",
    "    \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model_, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model_.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    display_topics(lsa, vectorizer.get_feature_names(),20)\n",
    "    \n",
    "    \n",
    "    return vectorizer, vect_data, lsa, lsa_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "vect_mod, vect_data, lsa_mod, lsa_data  = topic_mod_lsa(cleaned_talks,\n",
    "                                                        topics=20,\n",
    "                                                        ngram_min=1, \n",
    "                                                        ngram_max=2, \n",
    "                                                        max_df=0.6, \n",
    "                                                        max_feats=2000)\n",
    "print(\"LSA done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(lsa_data, axis=1)\n",
    "topic_labels = pd.DataFrame(topic_ind)\n",
    "topic_names = topic_labels\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804],10, topic_names, ted_all,\n",
    "                                       lsa_mod, vect_mod, lsa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mod_lsa_t(data, topics=5,ngram_min=1, ngram_max=3, max_df=0.6, max_feats=5000):\n",
    "    \n",
    "    \"\"\" vectorizer - turn words into numbers for each document(rows)\n",
    "    then use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(ngram_min,ngram_max), \n",
    "                             stop_words='english', \n",
    "                             max_df = max_df, \n",
    "                             max_features=max_feats)\n",
    "    \n",
    "    vect_data = vectorizer.fit_transform(data)\n",
    "    \n",
    "    stdScale = Normalizer()\n",
    "\n",
    "    vect_scale = stdScale.fit_transform(vect_data)\n",
    "    lsa_t = TruncatedSVD(n_components=topics,random_state=42)\n",
    "    \n",
    "    lsa_t_dat = lsa_t.fit_transform(vect_scale)\n",
    "        \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model_, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model_.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    display_topics(lsa_t, vectorizer.get_feature_names(),20)\n",
    "    \n",
    "    \n",
    "    return vectorizer, vect_data, lsa_t, lsa_t_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "vect_mod, vect_data, lsa_t_mod, lsa_t_data  = topic_mod_lsa_t(cleaned_talks,\n",
    "                                                              topics=20,\n",
    "                                                              ngram_min=1, \n",
    "                                                              ngram_max=2, \n",
    "                                                              max_df=0.6, \n",
    "                                                              max_feats=2000)\n",
    "print(\"LSA_T done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ind = np.argmax(lsa_t_data, axis=1)\n",
    "topic_labels = pd.DataFrame(topic_ind)\n",
    "topic_names = topic_labels\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(first_article,num_of_recs,topics,ted_data, model, vectorizer, training_vectors):\n",
    "    \n",
    "    new_vec = model.transform(\n",
    "        vectorizer.transform([first_article]))\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=num_of_recs, metric='cosine', algorithm='brute')\n",
    "    nn.fit(training_vectors)\n",
    "    \n",
    "    results = nn.kneighbors(new_vec)\n",
    "    \n",
    "    recommend_list = results[1][0]\n",
    "    scores = results[0]\n",
    "                       \n",
    "    ss = np.array(scores).flat\n",
    "    for i, resp in enumerate(recommend_list):\n",
    "        print('\\n--- ID ---\\n', + resp)\n",
    "        print('--- distance ---\\n', + ss[i])  \n",
    "        print('--- topic ---')\n",
    "        print(topics.iloc[resp,0])\n",
    "        print(ted_data.iloc[resp,1])\n",
    "        print('--- teds tags ---')\n",
    "        print(ted_data.iloc[resp,-3])\n",
    "        \n",
    "    return recommend_list, ss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804],10, topic_names, ted_all,\n",
    "                                       lsa_t_mod, vect_mod, lsa_t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
